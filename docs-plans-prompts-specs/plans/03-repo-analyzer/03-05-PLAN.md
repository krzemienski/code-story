# Plan 03-05: Key Component Identification and Dependency Mapping

## Overview

**Phase**: 3 - Repo Analyzer Agent
**Plan**: 05 of 05
**Depends on**: 03-03 (AST Analysis), 03-04 (Pattern Recognition)
**Enables**: Phase 4 (Story Architect Agent)

## Goal

Create tools that identify the most important components in a codebase and map dependencies between modules. This produces a prioritized list of what to explain in the narrative and visualizes how components interact.

## Claude Agent SDK Pattern

Uses `@tool` decorator for MCP tool definitions:

```python
from claude_agent_sdk import tool, create_sdk_mcp_server

@tool(
    name="identify_key_components",
    description="Identify and rank key components by importance",
    input_schema={...}
)
async def identify_key_components(args: dict) -> dict:
    ...
    return {"content": [{"type": "text", "text": json.dumps(result)}]}
```

## Tasks

### Task 1: Key Component Identification Tools

Create tools that rank components by importance based on multiple signals.

**File**: `src/codestory/tools/components.py`

```python
"""Key component identification and importance ranking tools.

Uses Claude Agent SDK @tool decorator for MCP integration.
"""

import json
from enum import Enum
from typing import Any

from claude_agent_sdk import tool


class ComponentType(str, Enum):
    """Types of code components."""
    ENTRY_POINT = "entry_point"
    CORE_MODULE = "core_module"
    API_ENDPOINT = "api_endpoint"
    DATA_MODEL = "data_model"
    SERVICE = "service"
    UTILITY = "utility"
    CONFIGURATION = "configuration"
    TEST = "test"


# Importance signals and weights
IMPORTANCE_SIGNALS = {
    "is_entry_point": 0.25,
    "high_import_count": 0.20,
    "is_api_endpoint": 0.15,
    "is_data_model": 0.15,
    "has_documentation": 0.10,
    "file_size": 0.08,
    "is_in_core_directory": 0.07,
}

ENTRY_POINT_PATTERNS = [
    "main.py", "__main__.py", "app.py", "server.py", "wsgi.py", "asgi.py",
    "manage.py", "cli.py", "run.py", "index.py", "index.ts", "index.js",
    "main.ts", "main.js", "App.tsx", "App.jsx",
]

CORE_DIRECTORY_PATTERNS = [
    "src/", "lib/", "core/", "app/", "api/", "services/", "domain/",
]

API_PATTERNS = ["routes", "routers", "endpoints", "views", "controllers", "api"]
MODEL_PATTERNS = ["models", "schemas", "entities", "domain", "types"]


def _normalize_import(import_str: str) -> str:
    """Normalize import string to module name."""
    if " import " in import_str:
        return import_str.split(" import ")[0].replace("from ", "").strip()
    return import_str.replace("import ", "").strip().split(".")[0]


def _path_to_module(file_path: str) -> str:
    """Convert file path to module name."""
    module = file_path.replace("/", ".").replace("\\", ".")
    for ext in [".py", ".ts", ".js", ".tsx", ".jsx"]:
        module = module.replace(ext, "")
    for prefix in ["src.", "lib.", "app."]:
        if module.startswith(prefix):
            module = module[len(prefix):]
    return module


def _get_type_distribution(components: list[dict]) -> dict[str, int]:
    """Get distribution of component types."""
    distribution: dict[str, int] = {}
    for comp in components:
        type_name = comp.get("component_type", "utility")
        distribution[type_name] = distribution.get(type_name, 0) + 1
    return distribution


@tool(
    name="identify_key_components",
    description="Analyze codebase to identify and rank the most important components based on multiple signals like imports, entry points, and structural position. Returns ranked list of components with importance scores and reasons.",
    input_schema={
        "type": "object",
        "properties": {
            "files": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "path": {"type": "string"},
                        "size": {"type": "integer"},
                        "has_docstring": {"type": "boolean"}
                    }
                },
                "description": "List of file info objects with path, size, has_docstring"
            },
            "imports_by_file": {
                "type": "object",
                "additionalProperties": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "description": "Map of file path to list of imports"
            },
            "classes_by_file": {
                "type": "object",
                "additionalProperties": {
                    "type": "array",
                    "items": {"type": "object"}
                },
                "description": "Map of file path to list of class definitions"
            },
            "functions_by_file": {
                "type": "object",
                "additionalProperties": {
                    "type": "array",
                    "items": {"type": "object"}
                },
                "description": "Map of file path to list of function definitions"
            }
        },
        "required": ["files", "imports_by_file"]
    }
)
async def identify_key_components(args: dict) -> dict:
    """Identify and rank key components in the codebase."""
    files = args.get("files", [])
    imports_by_file = args.get("imports_by_file", {})
    classes_by_file = args.get("classes_by_file", {})
    functions_by_file = args.get("functions_by_file", {})

    components: list[dict] = []

    # Calculate import frequency
    import_frequency: dict[str, int] = {}
    for file_path, imports in imports_by_file.items():
        for imp in imports:
            module = _normalize_import(imp)
            import_frequency[module] = import_frequency.get(module, 0) + 1

    max_imports = max(import_frequency.values()) if import_frequency else 1

    for file_info in files:
        file_path = file_info.get("path", "")
        file_name = file_path.split("/")[-1]

        # Skip test files
        if "test" in file_path.lower() and "test" not in file_name.lower():
            continue

        score = 0.0
        reasons = []
        component_type = ComponentType.UTILITY.value

        # Check if entry point
        if any(ep in file_name.lower() for ep in ENTRY_POINT_PATTERNS):
            score += IMPORTANCE_SIGNALS["is_entry_point"]
            reasons.append("Entry point file")
            component_type = ComponentType.ENTRY_POINT.value

        # Check import frequency
        module_name = _path_to_module(file_path)
        import_count = import_frequency.get(module_name, 0)
        if import_count > 0:
            normalized = import_count / max_imports
            score += IMPORTANCE_SIGNALS["high_import_count"] * normalized
            if import_count >= 3:
                reasons.append(f"Imported by {import_count} other modules")

        # Check if API endpoint
        if any(pattern in file_path.lower() for pattern in API_PATTERNS):
            score += IMPORTANCE_SIGNALS["is_api_endpoint"]
            reasons.append("API/endpoint definition")
            component_type = ComponentType.API_ENDPOINT.value

        # Check if data model
        if any(pattern in file_path.lower() for pattern in MODEL_PATTERNS):
            score += IMPORTANCE_SIGNALS["is_data_model"]
            reasons.append("Data model definition")
            component_type = ComponentType.DATA_MODEL.value

        # Check documentation
        if file_info.get("has_docstring", False):
            score += IMPORTANCE_SIGNALS["has_documentation"]
            reasons.append("Has documentation")

        # File size
        file_size = file_info.get("size", 0)
        if file_size > 1000:
            size_score = min(file_size / 10000, 1.0) * IMPORTANCE_SIGNALS["file_size"]
            score += size_score

        # Core directory bonus
        if any(pattern in file_path.lower() for pattern in CORE_DIRECTORY_PATTERNS):
            score += IMPORTANCE_SIGNALS["is_in_core_directory"]
            reasons.append("Located in core directory")

        # Determine type from classes
        classes = classes_by_file.get(file_path, [])
        functions = functions_by_file.get(file_path, [])

        if classes:
            class_names = [c.get("name", "").lower() for c in classes]
            if any("service" in name for name in class_names):
                component_type = ComponentType.SERVICE.value
            elif any("model" in name or "entity" in name for name in class_names):
                component_type = ComponentType.DATA_MODEL.value

        file_imports = imports_by_file.get(file_path, [])

        components.append({
            "name": file_name,
            "file_path": file_path,
            "component_type": component_type,
            "importance_score": min(score, 1.0),
            "reasons": reasons,
            "dependencies": file_imports[:10],
            "metrics": {
                "classes": len(classes),
                "functions": len(functions),
                "import_count": import_count,
                "file_size": file_size,
            },
        })

    # Sort by importance
    components.sort(key=lambda x: x["importance_score"], reverse=True)

    entry_points = [c for c in components if c["component_type"] == ComponentType.ENTRY_POINT.value]
    core_modules = [c for c in components if c["importance_score"] >= 0.5]

    result = {
        "components": components[:30],
        "entry_points": entry_points,
        "core_modules": core_modules[:10],
        "total_analyzed": len(components),
        "component_type_distribution": _get_type_distribution(components),
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}


def _is_internal_module(module: str, internal_modules: list[str]) -> bool:
    """Check if module is internal to the project."""
    module_lower = module.lower()
    for internal in internal_modules:
        if module_lower == internal.lower() or module_lower.startswith(internal.lower() + "."):
            return True
    external_prefixes = [
        "os", "sys", "re", "json", "typing", "dataclasses", "enum",
        "collections", "itertools", "functools", "pathlib", "datetime",
        "numpy", "pandas", "requests", "flask", "django", "fastapi",
        "sqlalchemy", "pydantic", "pytest", "unittest",
    ]
    return not any(module_lower.startswith(ext) for ext in external_prefixes)


@tool(
    name="build_dependency_graph",
    description="Build a dependency graph showing how modules import and depend on each other. Identifies entry points, leaf nodes, and hub nodes with high connectivity.",
    input_schema={
        "type": "object",
        "properties": {
            "imports_by_file": {
                "type": "object",
                "additionalProperties": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "module": {"type": "string"},
                            "items": {"type": "array", "items": {"type": "string"}},
                            "type": {"type": "string"}
                        }
                    }
                },
                "description": "Map of file path to list of import dicts with module, items, type"
            },
            "internal_modules": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of module names that are part of this project"
            }
        },
        "required": ["imports_by_file", "internal_modules"]
    }
)
async def build_dependency_graph(args: dict) -> dict:
    """Build dependency graph from import information."""
    imports_by_file = args.get("imports_by_file", {})
    internal_modules = args.get("internal_modules", [])

    nodes: set[str] = set()
    edges: list[dict] = []
    incoming_count: dict[str, int] = {}
    outgoing_count: dict[str, int] = {}

    for file_path, imports in imports_by_file.items():
        source_module = _path_to_module(file_path)
        nodes.add(source_module)

        for imp in imports:
            if isinstance(imp, dict):
                target_module = imp.get("module", "")
                import_type = imp.get("type", "direct")
                items = imp.get("items", [])
            else:
                target_module = str(imp)
                import_type = "direct"
                items = []

            if not _is_internal_module(target_module, internal_modules):
                continue

            nodes.add(target_module)
            edges.append({
                "source": source_module,
                "target": target_module,
                "import_type": import_type,
                "items_imported": items[:5],
            })

            outgoing_count[source_module] = outgoing_count.get(source_module, 0) + 1
            incoming_count[target_module] = incoming_count.get(target_module, 0) + 1

    entry_points = [
        n for n in nodes
        if incoming_count.get(n, 0) == 0 and outgoing_count.get(n, 0) > 0
    ]

    leaf_nodes = [
        n for n in nodes
        if outgoing_count.get(n, 0) == 0 and incoming_count.get(n, 0) > 0
    ]

    hub_threshold = len(edges) / len(nodes) * 2 if nodes else 0
    hub_nodes = [
        n for n in nodes
        if (incoming_count.get(n, 0) + outgoing_count.get(n, 0)) > hub_threshold
    ]

    result = {
        "nodes": list(nodes),
        "edges": edges,
        "entry_points": entry_points[:5],
        "leaf_nodes": leaf_nodes[:10],
        "hub_nodes": hub_nodes[:5],
        "node_count": len(nodes),
        "edge_count": len(edges),
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}


def _generate_coupling_recommendations(
    metrics: list[dict],
    highly_coupled: list[dict],
    avg_instability: float,
) -> list[str]:
    """Generate recommendations based on coupling analysis."""
    recommendations = []

    if len(highly_coupled) > 3:
        recommendations.append(
            f"Found {len(highly_coupled)} highly coupled modules. "
            "Consider introducing abstractions or interfaces to reduce coupling."
        )

    if avg_instability > 0.7:
        recommendations.append(
            f"Average instability is high ({avg_instability:.2f}). "
            "Many modules depend heavily on others. Consider stabilizing core modules."
        )
    elif avg_instability < 0.3:
        recommendations.append(
            f"Average instability is low ({avg_instability:.2f}). "
            "Architecture appears stable with clear dependency direction."
        )

    for m in metrics[:5]:
        if m["afferent_coupling"] > 10:
            recommendations.append(
                f"Module '{m['module']}' has high afferent coupling ({m['afferent_coupling']}). "
                "Consider if this module has too many responsibilities."
            )
            break

    if not recommendations:
        recommendations.append("Coupling metrics appear healthy. No immediate concerns.")

    return recommendations


@tool(
    name="analyze_module_coupling",
    description="Analyze coupling between modules to identify tightly coupled components and potential refactoring opportunities. Calculates afferent/efferent coupling and instability metrics.",
    input_schema={
        "type": "object",
        "properties": {
            "dependency_graph": {
                "type": "object",
                "properties": {
                    "nodes": {"type": "array", "items": {"type": "string"}},
                    "edges": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "source": {"type": "string"},
                                "target": {"type": "string"}
                            }
                        }
                    }
                },
                "description": "Dependency graph from build_dependency_graph"
            }
        },
        "required": ["dependency_graph"]
    }
)
async def analyze_module_coupling(args: dict) -> dict:
    """Analyze module coupling from dependency graph."""
    dependency_graph = args.get("dependency_graph", {})
    edges = dependency_graph.get("edges", [])
    nodes = dependency_graph.get("nodes", [])

    module_incoming: dict[str, list[str]] = {}
    module_outgoing: dict[str, list[str]] = {}

    for edge in edges:
        source = edge.get("source", "")
        target = edge.get("target", "")

        if target not in module_incoming:
            module_incoming[target] = []
        module_incoming[target].append(source)

        if source not in module_outgoing:
            module_outgoing[source] = []
        module_outgoing[source].append(target)

    coupling_metrics: list[dict] = []

    for node in nodes:
        ca = len(module_incoming.get(node, []))
        ce = len(module_outgoing.get(node, []))
        instability = ce / (ca + ce) if (ca + ce) > 0 else 0.5

        coupling_metrics.append({
            "module": node,
            "afferent_coupling": ca,
            "efferent_coupling": ce,
            "instability": round(instability, 2),
            "dependents": module_incoming.get(node, [])[:5],
            "dependencies": module_outgoing.get(node, [])[:5],
        })

    coupling_metrics.sort(
        key=lambda x: x["afferent_coupling"] + x["efferent_coupling"],
        reverse=True,
    )

    high_coupling_threshold = 5
    highly_coupled = [
        m for m in coupling_metrics
        if m["afferent_coupling"] + m["efferent_coupling"] >= high_coupling_threshold
    ]

    avg_instability = (
        sum(m["instability"] for m in coupling_metrics) / len(coupling_metrics)
        if coupling_metrics else 0
    )

    result = {
        "coupling_metrics": coupling_metrics[:20],
        "highly_coupled_modules": highly_coupled[:10],
        "average_instability": round(avg_instability, 2),
        "total_dependencies": len(edges),
        "recommendations": _generate_coupling_recommendations(
            coupling_metrics, highly_coupled, avg_instability
        ),
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}
```

### Task 2: Result Assembly Tools

Create tools that assemble all analysis into a comprehensive result structure for the Story Architect.

**File**: `src/codestory/tools/result_assembler.py`

```python
"""Assembles complete analysis results for Story Architect consumption.

Uses Claude Agent SDK @tool decorator for MCP integration.
"""

import json
from datetime import datetime
from typing import Any

from claude_agent_sdk import tool


def _generate_story_hooks(
    pattern_analysis: dict,
    component_analysis: dict,
    ast_analysis: dict,
) -> list[str]:
    """Generate narrative hooks based on analysis."""
    hooks = []

    arch_patterns = pattern_analysis.get("architectural_patterns", [])
    if arch_patterns:
        top = arch_patterns[0]
        hooks.append(
            f"The codebase follows a {top.get('pattern_name', '')} pattern, "
            f"which shapes how data flows through the system"
        )

    entry_points = component_analysis.get("entry_points", [])
    if entry_points:
        entry = entry_points[0]
        hooks.append(
            f"Our journey begins at {entry.get('name', 'the main entry point')}, "
            f"where the application comes to life"
        )

    complexity = ast_analysis.get("average_complexity", 0)
    if complexity > 10:
        hooks.append(
            "Some sophisticated algorithms lurk within, handling complex business logic"
        )

    return hooks[:5]


def _suggest_focus_areas(
    component_analysis: dict,
    pattern_analysis: dict,
) -> list[str]:
    """Suggest areas to focus on in the narrative."""
    focus_areas = []

    key_comps = component_analysis.get("core_modules", [])
    for comp in key_comps[:3]:
        focus_areas.append(comp.get("file_path", ""))

    design_patterns = pattern_analysis.get("design_patterns", [])
    for pattern in design_patterns[:2]:
        if pattern.get("confidence", 0) > 0.5:
            focus_areas.append(f"Pattern: {pattern.get('pattern_name', '')}")

    return focus_areas[:5]


def _generate_highlights(
    pattern_analysis: dict,
    dependency_analysis: dict,
    ast_analysis: dict,
) -> list[str]:
    """Generate technical highlights."""
    highlights = []

    arch_patterns = pattern_analysis.get("architectural_patterns", [])
    if arch_patterns:
        highlights.append(
            f"Implements {arch_patterns[0].get('pattern_name', '')} architecture"
        )

    design_patterns = pattern_analysis.get("design_patterns", [])
    if design_patterns:
        patterns = [p.get("pattern_name", "") for p in design_patterns[:3]]
        highlights.append(f"Uses design patterns: {', '.join(patterns)}")

    coupling = dependency_analysis.get("coupling", {})
    avg_instability = coupling.get("average_instability", 0.5)
    if avg_instability < 0.4:
        highlights.append("Well-structured with stable core modules")

    return highlights[:5]


@tool(
    name="assemble_analysis_result",
    description="Combine all analysis outputs into a comprehensive result structure optimized for narrative generation. Includes metadata, architecture, components, dependencies, and narrative hints.",
    input_schema={
        "type": "object",
        "properties": {
            "repo_url": {
                "type": "string",
                "description": "Repository URL"
            },
            "repo_name": {
                "type": "string",
                "description": "Repository name"
            },
            "structure_analysis": {
                "type": "object",
                "description": "Output from repository structure analysis"
            },
            "pattern_analysis": {
                "type": "object",
                "description": "Output from pattern recognition"
            },
            "component_analysis": {
                "type": "object",
                "description": "Output from key component identification"
            },
            "dependency_analysis": {
                "type": "object",
                "description": "Output from dependency mapping"
            },
            "ast_analysis": {
                "type": "object",
                "description": "Output from AST analysis"
            }
        },
        "required": ["repo_url", "repo_name", "structure_analysis", "pattern_analysis", "component_analysis"]
    }
)
async def assemble_analysis_result(args: dict) -> dict:
    """Assemble all analysis into final result."""
    repo_url = args.get("repo_url", "")
    repo_name = args.get("repo_name", "")
    structure_analysis = args.get("structure_analysis", {})
    pattern_analysis = args.get("pattern_analysis", {})
    component_analysis = args.get("component_analysis", {})
    dependency_analysis = args.get("dependency_analysis", {})
    ast_analysis = args.get("ast_analysis", {})

    file_count = structure_analysis.get("file_count", 0)
    directory_count = structure_analysis.get("directory_count", 0)
    languages = structure_analysis.get("languages", {})
    primary_language = max(languages.items(), key=lambda x: x[1])[0] if languages else "unknown"

    story_hooks = _generate_story_hooks(pattern_analysis, component_analysis, ast_analysis)
    suggested_focus = _suggest_focus_areas(component_analysis, pattern_analysis)
    highlights = _generate_highlights(pattern_analysis, dependency_analysis, ast_analysis)

    result = {
        "metadata": {
            "repo_url": repo_url,
            "repo_name": repo_name,
            "analyzed_at": datetime.utcnow().isoformat(),
        },
        "structure": {
            "file_count": file_count,
            "directory_count": directory_count,
            "primary_language": primary_language,
            "languages": languages,
        },
        "architecture": {
            "architectural_patterns": pattern_analysis.get("architectural_patterns", []),
            "design_patterns": pattern_analysis.get("design_patterns", []),
            "framework_patterns": pattern_analysis.get("framework_patterns", []),
        },
        "components": {
            "entry_points": component_analysis.get("entry_points", []),
            "key_components": component_analysis.get("core_modules", []),
        },
        "dependencies": {
            "graph": dependency_analysis.get("graph", {}),
            "coupling_analysis": dependency_analysis.get("coupling", {}),
        },
        "code_metrics": {
            "total_classes": ast_analysis.get("total_classes", 0),
            "total_functions": ast_analysis.get("total_functions", 0),
            "average_complexity": ast_analysis.get("average_complexity", 0.0),
        },
        "narrative_hints": {
            "story_hooks": story_hooks,
            "suggested_focus_areas": suggested_focus,
            "technical_highlights": highlights,
        },
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}


@tool(
    name="generate_analysis_summary",
    description="Generate a human-readable summary of the analysis for quick overview. Produces one-line summary and key statistics.",
    input_schema={
        "type": "object",
        "properties": {
            "analysis_result": {
                "type": "object",
                "description": "Complete analysis result dictionary from assemble_analysis_result"
            }
        },
        "required": ["analysis_result"]
    }
)
async def generate_analysis_summary(args: dict) -> dict:
    """Generate summary from complete analysis."""
    analysis_result = args.get("analysis_result", {})

    metadata = analysis_result.get("metadata", {})
    structure = analysis_result.get("structure", {})
    architecture = analysis_result.get("architecture", {})
    components = analysis_result.get("components", {})
    metrics = analysis_result.get("code_metrics", {})

    summary_points = []

    file_count = structure.get("file_count", 0)
    primary_lang = structure.get("primary_language", "unknown")
    summary_points.append(f"A {primary_lang} project with {file_count} files")

    arch_patterns = architecture.get("architectural_patterns", [])
    if arch_patterns:
        top_pattern = arch_patterns[0].get("pattern_name", "")
        summary_points.append(f"Following {top_pattern} architectural pattern")

    framework_patterns = architecture.get("framework_patterns", [])
    if framework_patterns:
        frameworks = [p.get("pattern_name", "") for p in framework_patterns[:2]]
        summary_points.append(f"Built with {', '.join(frameworks)}")

    entry_points = components.get("entry_points", [])
    key_comps = components.get("key_components", [])
    if entry_points:
        summary_points.append(
            f"{len(entry_points)} entry point(s) and {len(key_comps)} core modules"
        )

    classes = metrics.get("total_classes", 0)
    functions = metrics.get("total_functions", 0)
    summary_points.append(f"Contains {classes} classes and {functions} functions")

    result = {
        "repo_name": metadata.get("repo_name", ""),
        "one_line": summary_points[0] if summary_points else "Repository analyzed",
        "summary_points": summary_points,
        "key_stats": {
            "files": file_count,
            "classes": classes,
            "functions": functions,
            "language": primary_lang,
        },
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}
```

### Task 3: Create MCP Server for Component Tools

**File**: `src/codestory/tools/components_server.py`

```python
"""MCP server for component identification and dependency tools.

Uses Claude Agent SDK create_sdk_mcp_server.
"""

from claude_agent_sdk import create_sdk_mcp_server

from codestory.tools.components import (
    identify_key_components,
    build_dependency_graph,
    analyze_module_coupling,
)
from codestory.tools.result_assembler import (
    assemble_analysis_result,
    generate_analysis_summary,
)


def create_components_mcp_server():
    """Create MCP server with component analysis tools."""
    return create_sdk_mcp_server(
        name="components",
        version="1.0.0",
        tools=[
            identify_key_components,
            build_dependency_graph,
            analyze_module_coupling,
            assemble_analysis_result,
            generate_analysis_summary,
        ],
    )
```

### Task 4: Component Tools Tests

**File**: `tests/tools/test_components.py`

```python
"""Tests for component identification and dependency tools."""

import pytest
import json


@pytest.fixture
def sample_files():
    """Sample file data for testing."""
    return [
        {"path": "src/main.py", "size": 1500, "has_docstring": True},
        {"path": "src/api/routes.py", "size": 3000, "has_docstring": True},
        {"path": "src/models/user.py", "size": 800, "has_docstring": False},
        {"path": "src/utils/helpers.py", "size": 400, "has_docstring": False},
        {"path": "tests/test_main.py", "size": 500, "has_docstring": False},
    ]


@pytest.fixture
def sample_imports():
    """Sample import data for testing."""
    return {
        "src/main.py": ["from api.routes import router", "from models.user import User"],
        "src/api/routes.py": ["from models.user import User", "from utils.helpers import format"],
        "src/models/user.py": [],
        "src/utils/helpers.py": [],
    }


@pytest.mark.asyncio
async def test_identify_key_components(sample_files, sample_imports):
    """Test key component identification."""
    from codestory.tools.components import identify_key_components

    result = await identify_key_components({
        "files": sample_files,
        "imports_by_file": sample_imports,
        "classes_by_file": {},
        "functions_by_file": {},
    })

    assert "content" in result
    data = json.loads(result["content"][0]["text"])

    assert "components" in data
    assert "entry_points" in data
    assert data["total_analyzed"] > 0

    # main.py should be identified as entry point
    entry_names = [e["name"] for e in data["entry_points"]]
    assert "main.py" in entry_names


@pytest.mark.asyncio
async def test_build_dependency_graph(sample_imports):
    """Test dependency graph building."""
    from codestory.tools.components import build_dependency_graph

    result = await build_dependency_graph({
        "imports_by_file": {
            "src/main.py": [
                {"module": "api.routes", "items": ["router"], "type": "from"},
                {"module": "models.user", "items": ["User"], "type": "from"},
            ],
            "src/api/routes.py": [
                {"module": "models.user", "items": ["User"], "type": "from"},
            ],
        },
        "internal_modules": ["main", "api", "models", "utils"],
    })

    data = json.loads(result["content"][0]["text"])

    assert "nodes" in data
    assert "edges" in data
    assert data["node_count"] > 0


@pytest.mark.asyncio
async def test_analyze_module_coupling():
    """Test module coupling analysis."""
    from codestory.tools.components import analyze_module_coupling

    dependency_graph = {
        "nodes": ["main", "api.routes", "models.user", "utils.helpers"],
        "edges": [
            {"source": "main", "target": "api.routes"},
            {"source": "main", "target": "models.user"},
            {"source": "api.routes", "target": "models.user"},
            {"source": "api.routes", "target": "utils.helpers"},
        ],
    }

    result = await analyze_module_coupling({
        "dependency_graph": dependency_graph,
    })

    data = json.loads(result["content"][0]["text"])

    assert "coupling_metrics" in data
    assert "average_instability" in data
    assert "recommendations" in data

    # models.user should have high afferent coupling
    user_metrics = next(
        (m for m in data["coupling_metrics"] if "user" in m["module"]),
        None
    )
    assert user_metrics is not None
    assert user_metrics["afferent_coupling"] >= 2


@pytest.mark.asyncio
async def test_assemble_analysis_result():
    """Test analysis result assembly."""
    from codestory.tools.result_assembler import assemble_analysis_result

    result = await assemble_analysis_result({
        "repo_url": "https://github.com/test/repo",
        "repo_name": "test-repo",
        "structure_analysis": {
            "file_count": 50,
            "directory_count": 10,
            "languages": {"Python": 45, "JavaScript": 5},
        },
        "pattern_analysis": {
            "architectural_patterns": [{"pattern_name": "Layered", "confidence": 0.85}],
            "design_patterns": [{"pattern_name": "Repository", "confidence": 0.75}],
            "framework_patterns": [{"pattern_name": "FastAPI", "confidence": 0.9}],
        },
        "component_analysis": {
            "entry_points": [{"name": "main.py", "file_path": "src/main.py"}],
            "core_modules": [{"name": "api", "file_path": "src/api/"}],
        },
        "dependency_analysis": {},
        "ast_analysis": {"total_classes": 15, "total_functions": 80},
    })

    data = json.loads(result["content"][0]["text"])

    assert data["metadata"]["repo_name"] == "test-repo"
    assert data["structure"]["primary_language"] == "Python"
    assert len(data["architecture"]["architectural_patterns"]) > 0
    assert len(data["narrative_hints"]["story_hooks"]) > 0


@pytest.mark.asyncio
async def test_generate_analysis_summary():
    """Test analysis summary generation."""
    from codestory.tools.result_assembler import generate_analysis_summary

    analysis_result = {
        "metadata": {"repo_name": "test-repo"},
        "structure": {
            "file_count": 50,
            "primary_language": "Python",
        },
        "architecture": {
            "architectural_patterns": [{"pattern_name": "Layered"}],
            "framework_patterns": [{"pattern_name": "FastAPI"}],
        },
        "components": {
            "entry_points": [{"name": "main.py"}],
            "key_components": [{"name": "api"}],
        },
        "code_metrics": {
            "total_classes": 15,
            "total_functions": 80,
        },
    }

    result = await generate_analysis_summary({
        "analysis_result": analysis_result,
    })

    data = json.loads(result["content"][0]["text"])

    assert data["repo_name"] == "test-repo"
    assert "Python" in data["one_line"]
    assert len(data["summary_points"]) > 0
    assert data["key_stats"]["files"] == 50
```

## Files Created/Modified

| File | Action | Purpose |
|------|--------|---------|
| `src/codestory/tools/components.py` | Create | Key component identification tools |
| `src/codestory/tools/result_assembler.py` | Create | Analysis result assembly tools |
| `src/codestory/tools/components_server.py` | Create | MCP server for component tools |
| `tests/tools/test_components.py` | Create | Tests for component tools |

## Tool Naming Convention

When accessed via MCP, tools use the pattern: `mcp__components__<tool_name>`

- `mcp__components__identify_key_components`
- `mcp__components__build_dependency_graph`
- `mcp__components__analyze_module_coupling`
- `mcp__components__assemble_analysis_result`
- `mcp__components__generate_analysis_summary`

## Dependencies

- Plan 03-03 (AST Analysis) provides class/function data
- Plan 03-04 (Pattern Recognition) provides pattern analysis

## Validation Criteria

1. identify_key_components ranks entry points highest
2. build_dependency_graph correctly identifies internal vs external dependencies
3. analyze_module_coupling calculates instability metrics correctly
4. assemble_analysis_result produces complete structure for Story Architect
5. generate_analysis_summary creates readable one-line summary
6. Story hooks are relevant and narratively interesting
7. All tools return proper `{"content": [...]}` format

## Notes

- Importance scoring uses configurable weights
- Coupling analysis follows established software metrics (Ca, Ce, Instability)
- Result structure designed specifically for Story Architect consumption
- Story hooks provide starting points for narrative generation
