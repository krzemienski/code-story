# Plan 05-03: Script Preparation

## Overview

**Phase**: 5 - Voice Director Agent
**Plan**: 03 of 04
**Depends on**: 05-02 (Voice Director Agent)
**Enables**: 05-04 (Audio Assembly)

## Goal

Implement script preparation tools using Claude Agent SDK `@tool` decorator pattern for text chunking, voice marker processing, and ElevenLabs API limit management.

## Claude Agent SDK Pattern

Script preparation tools are implemented using `@tool` decorator:

```python
from claude_agent_sdk import tool

@tool(
    name="prepare_segment",
    description="Prepare a script segment for synthesis",
    input_schema={
        "type": "object",
        "properties": {...},
        "required": ["text"],
    },
)
async def prepare_segment(args: dict) -> dict:
    return {"content": [{"type": "text", "text": json.dumps(result)}]}
```

## Tasks

### Task 1: Create Script Preparation Tools

**File**: `src/codestory/voice/preparation.py`

```python
"""Script preparation tools for voice synthesis.

Handles text chunking, voice marker processing, and API limit management.
"""

import json
import re
from dataclasses import dataclass, asdict
from typing import Any

from claude_agent_sdk import tool


# ElevenLabs API limits
MAX_CHARS_PER_REQUEST = 5000   # Hard limit
OPTIMAL_CHUNK_SIZE = 2500      # Best for quality
MIN_CHUNK_SIZE = 100           # Minimum viable


@dataclass
class TextChunk:
    """A chunk of text prepared for synthesis."""
    text: str
    chunk_index: int
    original_segment_index: int
    is_continuation: bool
    pause_before_seconds: float = 0.0


@dataclass
class PreparedSegment:
    """A fully prepared segment for synthesis."""
    original_index: int
    segment_type: str
    chapter_number: int | None
    chunks: list[dict]
    total_characters: int
    voice_settings: dict[str, Any]
    pauses: list[dict]


# Voice direction marker patterns
PAUSE_PATTERN = re.compile(r'\[PAUSE\s+(\d+(?:\.\d+)?)\s*s?\]', re.IGNORECASE)
EMPHASIS_PATTERN = re.compile(r'\[EMPHASIS\s+([^\]]+)\]', re.IGNORECASE)
SLOW_PATTERN = re.compile(r'\[SLOW\](.*?)\[/SLOW\]', re.IGNORECASE | re.DOTALL)
CODE_PATTERN = re.compile(r'\[CODE\](.*?)\[/CODE\]', re.IGNORECASE | re.DOTALL)
FAST_PATTERN = re.compile(r'\[FAST\](.*?)\[/FAST\]', re.IGNORECASE | re.DOTALL)


def _extract_pauses(text: str) -> tuple[str, list[tuple[int, float]]]:
    """Extract pause markers and their positions.

    Returns:
        Tuple of (cleaned text, list of (position, duration) tuples)
    """
    pauses = []
    offset = 0

    for match in PAUSE_PATTERN.finditer(text):
        duration = float(match.group(1))
        position = match.start() - offset
        pauses.append((position, duration))
        offset += len(match.group(0))

    cleaned = PAUSE_PATTERN.sub('', text)
    return cleaned, pauses


def _process_emphasis(text: str) -> str:
    """Process emphasis markers for natural speech.

    ElevenLabs interprets emphasis naturally, so we just extract the word.
    """
    return EMPHASIS_PATTERN.sub(r'\1', text)


def _process_code_blocks(text: str) -> str:
    """Process code blocks for clearer pronunciation."""
    def format_code(match):
        code = match.group(1).strip()

        # Add spaces around operators for clearer reading
        code = re.sub(r'([._\-])', r' \1 ', code)

        # Spell out common programming constructs
        replacements = {
            '==': ' equals equals ',
            '!=': ' not equals ',
            '>=': ' greater than or equal to ',
            '<=': ' less than or equal to ',
            '->': ' arrow ',
            '=>': ' fat arrow ',
            '::': ' double colon ',
            '()': ' parentheses ',
            '[]': ' brackets ',
            '{}': ' braces ',
            '++': ' plus plus ',
            '--': ' minus minus ',
            '&&': ' and ',
            '||': ' or ',
            '...': ' dot dot dot ',
        }

        for old, new in replacements.items():
            code = code.replace(old, new)

        return code

    return CODE_PATTERN.sub(format_code, text)


def _process_slow_sections(text: str) -> str:
    """Process slow sections - remove markers, content handled by voice settings."""
    return SLOW_PATTERN.sub(r'\1', text)


def _process_fast_sections(text: str) -> str:
    """Process fast sections - remove markers."""
    return FAST_PATTERN.sub(r'\1', text)


def _find_sentence_boundary(text: str, max_pos: int) -> int:
    """Find the best sentence boundary before max_pos."""
    # Look for sentence endings
    sentence_endings = ['. ', '! ', '? ', '.\n', '!\n', '?\n']
    best_pos = -1

    search_text = text[:max_pos]
    for ending in sentence_endings:
        pos = search_text.rfind(ending)
        if pos > best_pos:
            best_pos = pos + len(ending)

    # If no sentence boundary, look for clause boundaries
    if best_pos < MIN_CHUNK_SIZE:
        clause_endings = [', ', '; ', ': ', ' - ', '\n']
        for ending in clause_endings:
            pos = search_text.rfind(ending)
            if pos > best_pos:
                best_pos = pos + len(ending)

    # Last resort: find word boundary
    if best_pos < MIN_CHUNK_SIZE:
        pos = search_text.rfind(' ')
        if pos > 0:
            best_pos = pos + 1

    return best_pos if best_pos > 0 else max_pos


def _chunk_text(text: str, max_size: int = OPTIMAL_CHUNK_SIZE) -> list[str]:
    """Split text into chunks at natural boundaries."""
    if len(text) <= max_size:
        return [text]

    chunks = []
    remaining = text

    while remaining:
        if len(remaining) <= max_size:
            chunks.append(remaining)
            break

        # Find best split point
        split_pos = _find_sentence_boundary(remaining, max_size)

        chunk = remaining[:split_pos].strip()
        if chunk:
            chunks.append(chunk)

        remaining = remaining[split_pos:].strip()

    return chunks


@tool(
    name="process_voice_markers",
    description="Process voice direction markers in text, extracting pauses and formatting code",
    input_schema={
        "type": "object",
        "properties": {
            "text": {
                "type": "string",
                "description": "Raw text with voice markers",
            },
        },
        "required": ["text"],
    },
)
async def process_voice_markers(args: dict) -> dict:
    """Process all voice direction markers in text."""
    text = args["text"]

    # Process markers in order
    processed = _process_emphasis(text)
    processed = _process_code_blocks(processed)
    processed = _process_slow_sections(processed)
    processed = _process_fast_sections(processed)

    # Extract pauses (stored separately for audio assembly)
    cleaned, pauses = _extract_pauses(processed)

    result = {
        "success": True,
        "processed_text": cleaned,
        "original_length": len(text),
        "processed_length": len(cleaned),
        "pauses": [{"position": p, "duration": d} for p, d in pauses],
        "markers_processed": {
            "emphasis": len(EMPHASIS_PATTERN.findall(text)),
            "code_blocks": len(CODE_PATTERN.findall(text)),
            "slow_sections": len(SLOW_PATTERN.findall(text)),
            "fast_sections": len(FAST_PATTERN.findall(text)),
            "pauses": len(pauses),
        },
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}


@tool(
    name="chunk_text_for_synthesis",
    description="Split text into chunks that respect ElevenLabs API limits",
    input_schema={
        "type": "object",
        "properties": {
            "text": {
                "type": "string",
                "description": "Text to chunk (should have markers already processed)",
            },
            "max_chunk_size": {
                "type": "integer",
                "description": "Maximum characters per chunk",
                "default": 2500,
            },
            "segment_index": {
                "type": "integer",
                "description": "Index of the original segment",
                "default": 0,
            },
        },
        "required": ["text"],
    },
)
async def chunk_text_for_synthesis(args: dict) -> dict:
    """Split text into synthesis-ready chunks."""
    text = args["text"]
    max_size = min(args.get("max_chunk_size", OPTIMAL_CHUNK_SIZE), MAX_CHARS_PER_REQUEST)
    segment_index = args.get("segment_index", 0)

    chunks = _chunk_text(text, max_size)

    chunk_objects = []
    for i, chunk_text in enumerate(chunks):
        chunk_objects.append({
            "text": chunk_text,
            "chunk_index": i,
            "original_segment_index": segment_index,
            "is_continuation": i > 0,
            "character_count": len(chunk_text),
            "within_limit": len(chunk_text) <= MAX_CHARS_PER_REQUEST,
        })

    result = {
        "success": True,
        "chunks": chunk_objects,
        "chunk_count": len(chunks),
        "total_characters": sum(len(c) for c in chunks),
        "all_within_limit": all(len(c) <= MAX_CHARS_PER_REQUEST for c in chunks),
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}


@tool(
    name="prepare_segment",
    description="Fully prepare a script segment for synthesis (process markers and chunk)",
    input_schema={
        "type": "object",
        "properties": {
            "text": {
                "type": "string",
                "description": "Raw segment text with markers",
            },
            "segment_type": {
                "type": "string",
                "description": "Type of segment",
                "enum": ["intro", "chapter", "transition", "outro"],
                "default": "chapter",
            },
            "segment_index": {
                "type": "integer",
                "description": "Index in the script",
                "default": 0,
            },
            "chapter_number": {
                "type": "integer",
                "description": "Chapter number if applicable",
            },
            "voice_settings": {
                "type": "object",
                "description": "Voice synthesis settings override",
            },
        },
        "required": ["text"],
    },
)
async def prepare_segment(args: dict) -> dict:
    """Fully prepare a segment for synthesis."""
    text = args["text"]
    segment_type = args.get("segment_type", "chapter")
    segment_index = args.get("segment_index", 0)
    chapter_number = args.get("chapter_number")
    voice_settings = args.get("voice_settings", {})

    # Process markers
    processed = _process_emphasis(text)
    processed = _process_code_blocks(processed)
    processed = _process_slow_sections(processed)
    processed = _process_fast_sections(processed)

    # Extract pauses
    cleaned, pauses = _extract_pauses(processed)

    # Chunk the text
    text_chunks = _chunk_text(cleaned)

    # Build chunk objects with pause info
    chunks = []
    for i, chunk_text in enumerate(text_chunks):
        # Check if any pause falls near the start of this chunk
        pause_before = 0.0
        if pauses:
            char_pos = sum(len(c) for c in text_chunks[:i])
            for pos, duration in pauses:
                if abs(pos - char_pos) < 10:  # Near chunk boundary
                    pause_before = duration
                    break

        chunks.append({
            "text": chunk_text,
            "chunk_index": i,
            "original_segment_index": segment_index,
            "is_continuation": i > 0,
            "pause_before_seconds": pause_before,
            "character_count": len(chunk_text),
        })

    result = {
        "success": True,
        "original_index": segment_index,
        "segment_type": segment_type,
        "chapter_number": chapter_number,
        "chunks": chunks,
        "chunk_count": len(chunks),
        "total_characters": len(cleaned),
        "voice_settings": voice_settings,
        "pauses": [{"position": p, "duration": d} for p, d in pauses],
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}


@tool(
    name="prepare_full_script",
    description="Prepare all segments of a script for synthesis",
    input_schema={
        "type": "object",
        "properties": {
            "segments": {
                "type": "array",
                "description": "Script segments from Story Architect",
                "items": {
                    "type": "object",
                    "properties": {
                        "content": {"type": "string"},
                        "segment_type": {"type": "string"},
                        "chapter_number": {"type": "integer"},
                        "voice_direction": {"type": "object"},
                    },
                },
            },
        },
        "required": ["segments"],
    },
)
async def prepare_full_script(args: dict) -> dict:
    """Prepare an entire script for synthesis."""
    segments = args["segments"]

    prepared_segments = []
    total_chunks = 0
    total_characters = 0

    for i, segment in enumerate(segments):
        # Get segment properties
        text = segment.get("content", segment.get("text", ""))
        segment_type = segment.get("segment_type", "chapter")
        chapter_num = segment.get("chapter_number")
        voice_settings = segment.get("voice_direction", segment.get("voice_settings", {}))

        # Prepare segment
        result = await prepare_segment({
            "text": text,
            "segment_type": segment_type,
            "segment_index": i,
            "chapter_number": chapter_num,
            "voice_settings": voice_settings,
        })

        prepared = json.loads(result["content"][0]["text"])

        if prepared.get("success"):
            prepared_segments.append(prepared)
            total_chunks += prepared["chunk_count"]
            total_characters += prepared["total_characters"]

    result = {
        "success": True,
        "segments": prepared_segments,
        "total_segments": len(prepared_segments),
        "total_chunks": total_chunks,
        "total_characters": total_characters,
        "estimated_api_calls": total_chunks,
        "estimated_duration_minutes": round((total_characters / 5 / 150), 1),
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}


@tool(
    name="validate_prepared_script",
    description="Validate a prepared script is ready for synthesis",
    input_schema={
        "type": "object",
        "properties": {
            "prepared_script": {
                "type": "object",
                "description": "Output from prepare_full_script",
            },
        },
        "required": ["prepared_script"],
    },
)
async def validate_prepared_script(args: dict) -> dict:
    """Validate a prepared script before synthesis."""
    prepared = args["prepared_script"]

    issues = []
    warnings = []

    segments = prepared.get("segments", [])

    if not segments:
        issues.append("No segments in script")

    for seg in segments:
        seg_idx = seg.get("original_index", "?")

        for chunk in seg.get("chunks", []):
            text = chunk.get("text", "")
            chunk_idx = chunk.get("chunk_index", "?")

            # Check chunk size
            if len(text) > MAX_CHARS_PER_REQUEST:
                issues.append(
                    f"Chunk {chunk_idx} in segment {seg_idx} exceeds API limit "
                    f"({len(text)} > {MAX_CHARS_PER_REQUEST})"
                )

            # Check for empty chunks
            if len(text.strip()) < 10:
                warnings.append(f"Very short chunk in segment {seg_idx}")

            # Check for unprocessed markers
            unprocessed = []
            if PAUSE_PATTERN.search(text):
                unprocessed.append("PAUSE")
            if EMPHASIS_PATTERN.search(text):
                unprocessed.append("EMPHASIS")
            if CODE_PATTERN.search(text):
                unprocessed.append("CODE")
            if SLOW_PATTERN.search(text):
                unprocessed.append("SLOW")

            if unprocessed:
                warnings.append(
                    f"Unprocessed markers in segment {seg_idx}: {', '.join(unprocessed)}"
                )

        # Check segment has chunks
        if not seg.get("chunks"):
            issues.append(f"Segment {seg_idx} has no chunks")

    result = {
        "valid": len(issues) == 0,
        "issues": issues,
        "warnings": warnings,
        "stats": {
            "total_segments": len(segments),
            "total_chunks": prepared.get("total_chunks", 0),
            "total_characters": prepared.get("total_characters", 0),
            "estimated_api_calls": prepared.get("estimated_api_calls", 0),
        },
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}


@tool(
    name="get_preparation_stats",
    description="Get statistics about a prepared script",
    input_schema={
        "type": "object",
        "properties": {
            "prepared_script": {
                "type": "object",
                "description": "Output from prepare_full_script",
            },
        },
        "required": ["prepared_script"],
    },
)
async def get_preparation_stats(args: dict) -> dict:
    """Get detailed statistics about prepared script."""
    prepared = args["prepared_script"]
    segments = prepared.get("segments", [])

    # Count by segment type
    by_type = {}
    for seg in segments:
        seg_type = seg.get("segment_type", "unknown")
        if seg_type not in by_type:
            by_type[seg_type] = {"count": 0, "chunks": 0, "characters": 0}
        by_type[seg_type]["count"] += 1
        by_type[seg_type]["chunks"] += seg.get("chunk_count", 0)
        by_type[seg_type]["characters"] += seg.get("total_characters", 0)

    # Count pauses
    total_pauses = 0
    total_pause_duration = 0
    for seg in segments:
        for pause in seg.get("pauses", []):
            total_pauses += 1
            total_pause_duration += pause.get("duration", 0)

    result = {
        "success": True,
        "summary": {
            "total_segments": len(segments),
            "total_chunks": prepared.get("total_chunks", 0),
            "total_characters": prepared.get("total_characters", 0),
            "estimated_duration_minutes": prepared.get("estimated_duration_minutes", 0),
        },
        "by_segment_type": by_type,
        "pauses": {
            "total_count": total_pauses,
            "total_duration_seconds": total_pause_duration,
        },
        "api_estimate": {
            "api_calls_needed": prepared.get("total_chunks", 0),
            "max_chars_per_call": MAX_CHARS_PER_REQUEST,
            "optimal_chunk_size": OPTIMAL_CHUNK_SIZE,
        },
    }

    return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}
```

### Task 2: Create Preparation MCP Server

**File**: `src/codestory/voice/preparation_server.py`

```python
"""MCP server for script preparation tools."""

from claude_agent_sdk import create_sdk_mcp_server

from codestory.voice.preparation import (
    process_voice_markers,
    chunk_text_for_synthesis,
    prepare_segment,
    prepare_full_script,
    validate_prepared_script,
    get_preparation_stats,
)


def create_preparation_mcp_server():
    """Create MCP server with script preparation tools."""
    return create_sdk_mcp_server(
        name="preparation",
        version="1.0.0",
        tools=[
            process_voice_markers,
            chunk_text_for_synthesis,
            prepare_segment,
            prepare_full_script,
            validate_prepared_script,
            get_preparation_stats,
        ],
    )
```

### Task 3: Update Voice Package Exports

**File**: `src/codestory/voice/__init__.py` (update)

```python
"""Voice synthesis module for Code Story.

Provides ElevenLabs integration, Voice Director, and script preparation tools.
"""

from codestory.voice.elevenlabs import (
    synthesize_text,
    synthesize_with_style,
    get_elevenlabs_voices,
    check_elevenlabs_quota,
    VOICE_PRESETS,
    STYLE_DEFAULTS,
    ElevenLabsError,
)
from codestory.voice.server import create_voice_mcp_server
from codestory.voice.director import (
    estimate_synthesis_cost,
    synthesize_story_segments,
    retry_failed_segments,
    generate_voice_preview,
)
from codestory.voice.preparation import (
    process_voice_markers,
    chunk_text_for_synthesis,
    prepare_segment,
    prepare_full_script,
    validate_prepared_script,
    get_preparation_stats,
    MAX_CHARS_PER_REQUEST,
    OPTIMAL_CHUNK_SIZE,
)
from codestory.voice.preparation_server import create_preparation_mcp_server

__all__ = [
    # ElevenLabs tools
    "synthesize_text",
    "synthesize_with_style",
    "get_elevenlabs_voices",
    "check_elevenlabs_quota",
    # Director tools
    "estimate_synthesis_cost",
    "synthesize_story_segments",
    "retry_failed_segments",
    "generate_voice_preview",
    # Preparation tools
    "process_voice_markers",
    "chunk_text_for_synthesis",
    "prepare_segment",
    "prepare_full_script",
    "validate_prepared_script",
    "get_preparation_stats",
    # Constants
    "VOICE_PRESETS",
    "STYLE_DEFAULTS",
    "MAX_CHARS_PER_REQUEST",
    "OPTIMAL_CHUNK_SIZE",
    # Servers
    "create_voice_mcp_server",
    "create_preparation_mcp_server",
    # Exceptions
    "ElevenLabsError",
]
```

### Task 4: Create Preparation Tests

**File**: `tests/voice/test_preparation.py`

```python
"""Tests for script preparation tools."""

import pytest
import json

from codestory.voice.preparation import (
    process_voice_markers,
    chunk_text_for_synthesis,
    prepare_segment,
    prepare_full_script,
    validate_prepared_script,
    get_preparation_stats,
    MAX_CHARS_PER_REQUEST,
    OPTIMAL_CHUNK_SIZE,
)


@pytest.mark.asyncio
async def test_process_voice_markers():
    """Test voice marker processing."""
    text = "[PAUSE 2s] Welcome to [EMPHASIS Code Story]. [CODE]main()[/CODE] is the entry point."

    result = await process_voice_markers({"text": text})
    content = json.loads(result["content"][0]["text"])

    assert content["success"] is True
    assert "PAUSE" not in content["processed_text"]
    assert "EMPHASIS" not in content["processed_text"]
    assert "CODE" not in content["processed_text"]
    assert len(content["pauses"]) == 1
    assert content["pauses"][0]["duration"] == 2.0


@pytest.mark.asyncio
async def test_process_code_markers():
    """Test code block processing."""
    text = "The [CODE]==>[/CODE] operator is called fat arrow."

    result = await process_voice_markers({"text": text})
    content = json.loads(result["content"][0]["text"])

    assert "fat arrow" in content["processed_text"]


@pytest.mark.asyncio
async def test_chunk_text_short():
    """Test chunking short text (no split needed)."""
    text = "This is a short text that doesn't need chunking."

    result = await chunk_text_for_synthesis({"text": text})
    content = json.loads(result["content"][0]["text"])

    assert content["success"] is True
    assert content["chunk_count"] == 1
    assert content["all_within_limit"] is True


@pytest.mark.asyncio
async def test_chunk_text_long():
    """Test chunking long text."""
    # Create text longer than optimal chunk size
    text = "This is a sentence. " * 200  # ~4000 chars

    result = await chunk_text_for_synthesis({
        "text": text,
        "max_chunk_size": OPTIMAL_CHUNK_SIZE,
    })
    content = json.loads(result["content"][0]["text"])

    assert content["success"] is True
    assert content["chunk_count"] > 1
    assert content["all_within_limit"] is True

    # Verify all chunks are under limit
    for chunk in content["chunks"]:
        assert chunk["character_count"] <= MAX_CHARS_PER_REQUEST


@pytest.mark.asyncio
async def test_chunk_at_sentence_boundary():
    """Test that chunks split at sentence boundaries."""
    text = "First sentence. Second sentence. Third sentence. Fourth sentence."

    result = await chunk_text_for_synthesis({
        "text": text,
        "max_chunk_size": 40,  # Force split
    })
    content = json.loads(result["content"][0]["text"])

    # Each chunk should end at a sentence
    for chunk in content["chunks"]:
        text = chunk["text"].strip()
        if not chunk["is_continuation"]:  # First chunk of segment
            assert text.endswith('.') or text.endswith('!') or text.endswith('?') or len(text) <= 40


@pytest.mark.asyncio
async def test_prepare_segment():
    """Test full segment preparation."""
    text = "[PAUSE 1s] Welcome to Code Story. [EMPHASIS Important] concept here."

    result = await prepare_segment({
        "text": text,
        "segment_type": "intro",
        "segment_index": 0,
    })
    content = json.loads(result["content"][0]["text"])

    assert content["success"] is True
    assert content["segment_type"] == "intro"
    assert content["chunk_count"] >= 1
    assert len(content["pauses"]) == 1
    # Markers should be processed
    for chunk in content["chunks"]:
        assert "PAUSE" not in chunk["text"]
        assert "EMPHASIS" not in chunk["text"]


@pytest.mark.asyncio
async def test_prepare_full_script():
    """Test full script preparation."""
    segments = [
        {"content": "Welcome to the story.", "segment_type": "intro"},
        {"content": "Here is chapter one content.", "segment_type": "chapter", "chapter_number": 1},
        {"content": "Let's move on.", "segment_type": "transition"},
        {"content": "Thanks for listening.", "segment_type": "outro"},
    ]

    result = await prepare_full_script({"segments": segments})
    content = json.loads(result["content"][0]["text"])

    assert content["success"] is True
    assert content["total_segments"] == 4
    assert content["total_chunks"] >= 4
    assert content["total_characters"] > 0


@pytest.mark.asyncio
async def test_validate_prepared_script():
    """Test script validation."""
    # First prepare a script
    segments = [
        {"content": "Valid segment content.", "segment_type": "chapter"},
    ]
    prep_result = await prepare_full_script({"segments": segments})
    prepared = json.loads(prep_result["content"][0]["text"])

    # Then validate
    result = await validate_prepared_script({"prepared_script": prepared})
    content = json.loads(result["content"][0]["text"])

    assert content["valid"] is True
    assert len(content["issues"]) == 0


@pytest.mark.asyncio
async def test_validate_catches_oversized_chunk():
    """Test validation catches oversized chunks."""
    # Create a prepared script with oversized chunk (mock)
    prepared = {
        "segments": [{
            "original_index": 0,
            "segment_type": "chapter",
            "chunks": [{
                "text": "x" * 6000,  # Over limit
                "chunk_index": 0,
            }],
        }],
        "total_chunks": 1,
        "total_characters": 6000,
    }

    result = await validate_prepared_script({"prepared_script": prepared})
    content = json.loads(result["content"][0]["text"])

    assert content["valid"] is False
    assert len(content["issues"]) > 0
    assert "exceeds API limit" in content["issues"][0]


@pytest.mark.asyncio
async def test_get_preparation_stats():
    """Test statistics generation."""
    segments = [
        {"content": "Intro content.", "segment_type": "intro"},
        {"content": "Chapter one.", "segment_type": "chapter", "chapter_number": 1},
        {"content": "Chapter two.", "segment_type": "chapter", "chapter_number": 2},
    ]

    prep_result = await prepare_full_script({"segments": segments})
    prepared = json.loads(prep_result["content"][0]["text"])

    result = await get_preparation_stats({"prepared_script": prepared})
    content = json.loads(result["content"][0]["text"])

    assert content["success"] is True
    assert content["summary"]["total_segments"] == 3
    assert "intro" in content["by_segment_type"]
    assert "chapter" in content["by_segment_type"]
    assert content["by_segment_type"]["chapter"]["count"] == 2
```

## Files Created/Modified

| File | Action | Purpose |
|------|--------|---------|
| `src/codestory/voice/preparation.py` | Create | Script preparation tools |
| `src/codestory/voice/preparation_server.py` | Create | Preparation MCP server |
| `src/codestory/voice/__init__.py` | Update | Export preparation tools |
| `tests/voice/test_preparation.py` | Create | Preparation tests |

## Validation Criteria

1. All tools use `@tool` decorator with proper `input_schema`
2. `process_voice_markers` handles all marker types (PAUSE, EMPHASIS, CODE, SLOW, FAST)
3. `chunk_text_for_synthesis` splits at sentence boundaries
4. All chunks are within `MAX_CHARS_PER_REQUEST` (5000)
5. `validate_prepared_script` catches oversized chunks and unprocessed markers
6. `prepare_full_script` handles complete scripts
7. All tests pass

## Tool Naming Convention

Tools are accessed via MCP server as: `mcp__preparation__<tool_name>`

- `mcp__preparation__process_voice_markers`
- `mcp__preparation__chunk_text_for_synthesis`
- `mcp__preparation__prepare_segment`
- `mcp__preparation__prepare_full_script`
- `mcp__preparation__validate_prepared_script`
- `mcp__preparation__get_preparation_stats`

## Voice Marker Reference

| Marker | Purpose | Processing |
|--------|---------|------------|
| `[PAUSE Xs]` | Insert silence | Extracted, stored for audio assembly |
| `[EMPHASIS word]` | Emphasize word | Marker removed, ElevenLabs interprets naturally |
| `[CODE]...[/CODE]` | Code pronunciation | Operators spelled out (== -> "equals equals") |
| `[SLOW]...[/SLOW]` | Slower speech | Marker removed, handled by voice settings |
| `[FAST]...[/FAST]` | Faster speech | Marker removed, handled by voice settings |

## Next Step

Ready for 05-04-PLAN.md (Audio Assembly)
